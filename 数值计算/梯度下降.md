# 梯度下降

# Gradient Descent | 梯度下降

梯度下降法 Gradient Descent 是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。考虑无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数。如果能构造出一个序列$x^0,x^1,...,x^t$满足：

$$
f(x^{t+1}) < f(x^t),t=0,1,2...
$$

则不断执行该过程即可以收敛到局部极小点。而根据泰勒展示我们可以知道:

$$
f(x+\Delta x) \simeq f(x) + \Delta x^T \nabla f(x)
$$

于是，如果要满足 $f(x+\Delta x) < f(x)$，可以选择:

$$
\Delta x = -{step} \nabla f(x)
$$

其中$step$是一个小常数，表示步长。以求解目标函数最小化为例，梯度下降算法可能存在一下几种情况：

- 当目标函数为凸函数时，局部极小点就对应着函数全局最小值时，这种方法可以快速的找到最优解；
- 当目标函数存在多个局部最小值时，可能会陷入局部最优解。因此需要从多个随机的起点开始解的搜索。
- 当目标函数不存在最小值点，则可能陷入无限循环。因此，有必要设置最大迭代次数。

# Links

- https://mp.weixin.qq.com/s/VvdbyvUDIGUiIct8k0trvQ
