# KL 散度 & 相对熵

相对熵又称互熵，交叉熵，鉴别信息，Kullback 熵，Kullback-Leible 散度(即 KL 散度)等。设$p(x)$和$q(x)$是$x$取值的两个概率概率分布，则$p$对$q$的相对熵为

$$
D(p||q) = \sum\_{i=1}^{n}p(x_i)log\frac{p(x_i)}{q(x_i)}
$$

在一定程度上，熵可以度量两个随机变量的距离。KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度是用来度量使用基于 Q 的编码来编码来自 P 的样本平均所需的额外的位元数。典型情况下，P 表示数据的真实分布，Q 表示数据的理论分布，模型分布，或 P 的近似分布。
相对熵(KL 散度)有两个主要的性质。如下
(1)尽管 KL 散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即$$D(p||q) \neq D(q||p)$$
(2)相对熵的值为非负值，即

$$
D(p||q) > 0
$$

# 相对熵的应用

相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵(KL 散度)可以用于比较文本的相似度，先统计出词的频率，然后计算 KL 散度就行了。另外，在多指标系统评估中，指标权重分配是一个重点和难点，通过相对熵可以处理。

# 互信息

两个随机变量$X$，$Y$的互信息，定义为$X$，$Y$的联合分布和独立分布乘积的相对熵。
$$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$
$$I(X,Y)=\sum\_{x,y}log\frac{p(x,y)}{p(x)p(y)}$$

# 信息增益

信息增益表示得知特征 A 的信息而使得类$X$的信息的不确定性减少的程度。信息增益的定义为特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差：

$$g(D,A) = H(D) - H(D|A)$$
